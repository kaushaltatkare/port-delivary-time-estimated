# -*- coding: utf-8 -*-
"""Port delivaery2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sCUVbgb5_WBrmA5UDTQ057kTat0HmLHe
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

df=pd.read_csv('/content/dataset.csv')

df.head()

df.shape

df.info()

from datetime import datetime

df['created_at'] = pd.to_datetime(df['created_at'])
df['actual_delivery_time'] = pd.to_datetime(df['actual_delivery_time'],errors='coerce')

df['time_taken'] = df['actual_delivery_time']- df['created_at']

df['time_taken_mins']=pd.to_timedelta(df['time_taken'])/pd.Timedelta('60s')

df['hour'] = df['created_at'].dt.hour
df['day'] = df['created_at'].dt.dayofweek

df.head()

df.drop(['created_at','actual_delivery_time','time_taken','hour','day'],axis = 1, inplace= True)

df.isnull().sum()/ len(df) *100

df.shape

df.sample(5)

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='viridis')
plt.title('Heatmap of Data')
plt.show()

drop_columns = ['market_id', 'store_id','order_protocol']
df.drop(columns=drop_columns, inplace=True)

df.duplicated().sum()

df.drop_duplicates(inplace = True)

df.duplicated().sum()

df.dropna(inplace=True)

df.isnull().sum()

average_items_per_order = df['total_items'].mean()

print(f"Average Number of Items per Order: {average_items_per_order}")

most_common_category = df['store_primary_category'].max()
print(f'the most common category is : {most_common_category}')

plt.figure(figsize=(21, 5))
sns.countplot(x='store_primary_category', data=df, palette='Set3')
plt.xlabel('Store Category')
plt.ylabel('Number of Orders')
plt.title('Distribution of Orders Among Store Categories')
plt.xticks(rotation=90)
plt.show()

average_per_order = df['total_items'].median()
print(f'Average order is :{average_per_order}')

plt.figure(figsize=(10, 6))
sns.boxplot(data=df[['min_item_price', 'max_item_price']], palette='Set2')
plt.xlabel('Item Price')
plt.ylabel('Price')
plt.title('Variation in Minimum and Maximum Item Prices Across Orders')
plt.show()

df.describe()

numerical_columns = ['total_items', 'subtotal', 'num_distinct_items','min_item_price','max_item_price','total_onshift_partners','total_busy_partners','total_outstanding_orders']

for column in numerical_columns:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df[column])
    plt.title(f'{column} Distribution')
    plt.xticks(rotation=45)
    plt.show()

# Finding the IQR
percentile25 = df[['total_items',	'subtotal',	'num_distinct_items' , 'min_item_price',	'max_item_price',	'total_onshift_partners',	'total_busy_partners',	'total_outstanding_orders']].quantile(0.25)
percentile75 = df[['total_items',	'subtotal',	'num_distinct_items' , 'min_item_price',	'max_item_price',	'total_onshift_partners',	'total_busy_partners',	'total_outstanding_orders']].quantile(0.75)

iqr = percentile75 - percentile25

iqr

upper_limit = percentile75 + 1.5 * iqr
lower_limit = percentile25 - 1.5 * iqr

print("Upper limit",upper_limit)
print("Lower limit",lower_limit)

df[['total_items',	'subtotal',	'num_distinct_items' , 'min_item_price',	'max_item_price',	'total_onshift_partners',	'total_busy_partners',	'total_outstanding_orders']] = np.where(
    df[['total_items',	'subtotal',	'num_distinct_items' , 'min_item_price',	'max_item_price',	'total_onshift_partners',	'total_busy_partners',	'total_outstanding_orders']] > upper_limit,
    upper_limit,
    np.where(
        df[['total_items',	'subtotal',	'num_distinct_items' , 'min_item_price',	'max_item_price',	'total_onshift_partners',	'total_busy_partners',	'total_outstanding_orders']] < lower_limit,
        lower_limit,
        df[['total_items',	'subtotal',	'num_distinct_items' , 'min_item_price',	'max_item_price',	'total_onshift_partners',	'total_busy_partners',	'total_outstanding_orders']]
    )
)

df.shape

numerical_columns = ['total_items', 'subtotal', 'num_distinct_items','min_item_price','max_item_price','total_onshift_partners','total_busy_partners','total_outstanding_orders']

for column in numerical_columns:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df[column])
    plt.title(f'{column} Distribution')
    plt.xticks(rotation=45)
    plt.show()

numerical_columns = ['total_items', 'subtotal', 'num_distinct_items', 'min_item_price', 'max_item_price', 'total_onshift_partners', 'total_busy_partners', 'total_outstanding_orders']

plt.figure(figsize=(16, 8))

for i, column in enumerate(numerical_columns, 1):
    plt.subplot(2, 4, i)
    sns.histplot(df[column], kde=True)  # Using sns.histplot() instead of sns.distplot() as distplot() is deprecated
    plt.title(column)

plt.tight_layout()
plt.show()

X = df.drop('time_taken_mins',axis = 1)
y = df['time_taken_mins']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)

ohe = OneHotEncoder(drop = 'first', sparse=False)

ohe.fit(X_train[['store_primary_category']])

encoded_cols = list(ohe.get_feature_names_out())

X_train[encoded_cols] = ohe.transform(X_train[['store_primary_category']])
X_test[encoded_cols] = ohe.transform(X_test[['store_primary_category']])

X_train.drop('store_primary_category',axis=1,inplace=True)
X_test.drop('store_primary_category',axis=1,inplace=True)

X_train

df

columns_to_transform = ['total_items', 'subtotal', 'num_distinct_items', 'min_item_price', 'max_item_price', 'total_onshift_partners', 'total_busy_partners', 'total_outstanding_orders']
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[columns_to_transform])
X_test_scaled = scaler.transform(X_test[columns_to_transform])

X_train_scaled

model = LinearRegression()
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:",np.sqrt(mse))

mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)

1077.411968925728/60

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.metrics import mean_absolute_error

# Define the pipeline
pipeline = make_pipeline(
    StandardScaler(),
    PowerTransformer(),
    Ridge()  # Ridge Regression with regularization
)

# Define the hyperparameters to tune
param_grid = {
    'ridge__alpha': [0.1, 1.0, 10.0]  # Regularization strength
}

# Perform grid search cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_absolute_error')
grid_search.fit(X_train_scaled, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict on the test set
y_pred = best_model.predict(X_test_scaled)

# Calculate Mean Absolute Error
mae = mean_absolute_error(y_test, y_pred)
print("Best Mean Absolute Error after hyperparameter tuning:", mae)

# Get the best hyperparameters
best_parameters = grid_search.best_params_
print("Best Hyperparameters:", best_parameters)

from sklearn.model_selection import cross_val_score

# Perform cross-validation
cv_scores = cross_val_score(pipeline, X_train_scaled, y_train, cv=5, scoring='neg_mean_absolute_error')

# Convert scores to positive values
cv_scores = -cv_scores

# Print cross-validation scores
print("Cross-Validation Mean Absolute Error Scores:", cv_scores)
print("Average Mean Absolute Error:", cv_scores.mean())

